{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7cuX3lV6OT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
      ],
      "metadata": {
        "id": "qfG1qe8ZvtxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        " R-squared (R2) is popular as a statistical measure that represents the proportion of the variance for a dependent variable. It explained an independent variable or variables in a regression model. Whereas correlation explains the strength of the relationship between two variables independent and dependent variables. R-squared explains to what extent the variance of one variable explains the variance of the second one. So, if the R2 value of a model is 0.50, then approximately half of the observed variation will be explained by its inputs.\n",
        "\n",
        "In investing, R-squared is generally interpreted as the percentage of some fund or security movements which can be explained by movements in the benchmark index. For example, an R-squared for fixed-income security versus some specific bond index will identify the security’s proportion of price movement since it is predictable as per the price movement of the index. The same can be applied to the stock versus the S & P 500 index, or any other related relevant inde"
      ],
      "metadata": {
        "id": "aiD0VFeBvuvx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBn5M73gxSuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
      ],
      "metadata": {
        "id": "ldp8lwk7xXGm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DZ7JMePxYBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected. Typically, the adjusted R-squared is positive, not negative. It is always lower than the R-squared.\n",
        "\n",
        "Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. This is called overfitting and can return an unwarranted high R-squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n",
        "2\n",
        "\n",
        "In a portfolio model that has more independent variables, adjusted R-squared will help determine how much of the correlation with the index is due to the addition of those variables. The adjusted R-squared compensates for the addition of variables and only increases if the new predictor enhances the model above what would be obtained by probability. Conversely, it will decrease when a predictor improves the model less than what is predicted by chance.\n",
        "\n",
        "Key Differences\n",
        "The most obvious difference between adjusted R-squared and R-squared is simply that adjusted R-squared considers and tests different independent variables against the stock index and R-squared does not. Because of this, many investment professionals prefer using adjusted R-squared because it has the potential to be more accurate. Furthermore, investors can gain additional information about what is affecting a stock by testing various independent variables using the adjusted R-squared model.\n",
        "\n",
        "R-squared, on the other hand, does have its limitations. One of the most essential limits to using this model is that R-squared cannot be used to determine whether or not the coefficient estimates and predictions are biased. Furthermore,  in multiple linear regression, the R-squared cannot tell us which regression variable is more important than the other."
      ],
      "metadata": {
        "id": "PMCB-XDNxrG-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHjjCInhxs1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. When is it more appropriate to use adjusted R-squared?"
      ],
      "metadata": {
        "id": "89wOgjzax1Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0egk2m6x153"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "comparing R-squared and Adjusted R-squared provides insights into model fit and complexity. Residual Sum of Squares gauges the model’s accuracy, while understanding R-squared illuminates explained variance. Despite its utility, R-squared has limitations, addressed by the more nuanced Adjusted R-squared. Both metrics refine our assessment of regression models"
      ],
      "metadata": {
        "id": "QJg0SCIZyFYo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwZlPH_ayG0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
      ],
      "metadata": {
        "id": "ZCt6kgMtyKx6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7JDTZbthyMXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "RMSE (Root Mean Squared Error):\n",
        "\n",
        "Calculation: RMSE is the square root of the average of the squared differences between the predicted and actual values. It provides a measure of the average magnitude of the prediction errors in the same units as the dependent variable.\n",
        "Representation: RMSE is sensitive to large errors since it involves squaring the errors, and then taking the square root. It penalizes larger deviations more heavily compared to smaller ones.\n",
        "MSE (Mean Squared Error):\n",
        "\n",
        "Calculation: MSE is the average of the squared differences between the predicted and actual values. It is obtained by summing up the squared errors and dividing by the number of observations.\n",
        "Representation: Like RMSE, MSE measures the average magnitude of the squared errors. It provides a measure of the overall model accuracy, and a lower MSE indicates better model performance.\n",
        "MAE (Mean Absolute Error):\n",
        "\n",
        "Calculation: MAE is the average of the absolute differences between the predicted and actual values. It is obtained by summing up the absolute errors and dividing by the number of observations.\n",
        "Representation: MAE is less sensitive to outliers compared to RMSE. It provides a measure of the average absolute magnitude of the prediction errors. MAE is in the same units as the dependent variable, making it more interpretable.\n",
        "Interpretation:\n",
        "\n",
        "RMSE: A lower RMSE indicates better model performance, and it is commonly used when larger errors should be penalized more.\n",
        "\n",
        "MSE: Similar to RMSE, a lower MSE indicates better model performance. It is easier to work with mathematically than RMSE since it avoids the square root operation.\n",
        "\n",
        "MAE: MAE is more robust to outliers compared to RMSE. A lower MAE indicates better model performance, and it is suitable when the impact of larger errors should be minimized."
      ],
      "metadata": {
        "id": "3huQv5oyyWP4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVrdp4X0yX4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
      ],
      "metadata": {
        "id": "B3sSmmMQyZEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XY_KLjv7yeBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Ip_fD6OyfAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "#RMSE (Root Mean Squared Error):\n",
        "\n",
        "#Advantages:\n",
        "Sensitivity to Large Errors: RMSE is sensitive to large errors due to the squaring operation, which can be advantageous if large errors are considered more critical.\n",
        "Mathematical Properties: It has nice mathematical properties, and its use often simplifies certain mathematical operations.\n",
        "Disadvantages:\n",
        "Impact of Outliers: RMSE can be heavily influenced by outliers, as squaring the errors magnifies their effect.\n",
        "Interpretability: The square root operation may make the metric less intuitive for interpretation in the original units of the dependent variable.\n",
        "#MSE (Mean Squared Error):\n",
        "\n",
        "Advantages:\n",
        "Mathematical Simplicity: MSE is mathematically simpler than RMSE, making it easier to work with in certain calculations.\n",
        "Sensitivity to Errors: It penalizes larger errors more heavily than MAE, which may be appropriate in situations where large errors should be emphasized.\n",
        "Disadvantages:\n",
        "Same as RMSE: Shares some disadvantages with RMSE, such as sensitivity to outliers and potentially reduced interpretability.\n",
        "#MAE (Mean Absolute Error):\n",
        "\n",
        "Advantages:\n",
        "Robustness to Outliers: MAE is less sensitive to outliers compared to RMSE, making it a more robust metric in the presence of extreme values.\n",
        "Interpretability: MAE is directly interpretable in the original units of the dependent variable, which can enhance its practicality.\n",
        "Disadvantages:\n",
        "Less Sensitivity to Larger Errors: It treats all errors equally, which might be a disadvantage when larger errors should be penalized more."
      ],
      "metadata": {
        "id": "m7V056O0ypom"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mEB7fuQyrOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
      ],
      "metadata": {
        "id": "m9nED8bTy5VM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOBJquf9y7RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "Lasso Regularization:\n",
        "\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other regression models to prevent overfitting and handle multicollinearity by adding a penalty term to the objective function. The penalty term in Lasso is the absolute value of the coefficients multiplied by a regularization parameter (lambda or alpha). The key characteristic of Lasso regularization is that it tends to shrink some of the coefficients exactly to zero, effectively performing feature selection.\n",
        "\n",
        "Differences from Ridge Regularization:\n",
        "\n",
        "Penalty Term:\n",
        "\n",
        "Lasso: The penalty term in Lasso is the absolute value of the coefficients multiplied by the regularization parameter.\n",
        "Ridge: The penalty term in Ridge is the square of the coefficients multiplied by the regularization parameter.\n",
        "Effect on Coefficients:\n",
        "\n",
        "Lasso: Lasso has a tendency to drive some coefficients exactly to zero. This makes Lasso useful for feature selection, as it effectively eliminates some irrelevant or less important features.\n",
        "Ridge: Ridge tends to shrink the coefficients towards zero but rarely makes them exactly zero. It helps in dealing with multicollinearity by reducing the impact of correlated features.\n",
        "Feature Selection:\n",
        "\n",
        "Lasso: Lasso regularization inherently performs feature selection by setting some coefficients to zero. This makes it particularly useful when dealing with high-dimensional datasets with many features.\n",
        "Ridge: While Ridge helps with multicollinearity, it doesn't perform explicit feature selection in the same way as Lasso.\n",
        "Appropriateness of Lasso:\n",
        "\n",
        "Lasso regularization is more appropriate when:\n",
        "\n",
        "There is a belief that many of the features are irrelevant or contribute little to the model.\n",
        "Feature selection is desired, and the goal is to have a sparse model with fewer predictors.\n",
        "Dealing with high-dimensional datasets where the number of features is comparable to or larger than the number of observations."
      ],
      "metadata": {
        "id": "g4fRoeGFzIQV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4lHa5YTzJtg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}