{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GskMLNXOLRQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
      ],
      "metadata": {
        "id": "ZpAWbENRONpr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VGZHqaJlOV6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:- In the case of linear regression we find best fit line and we predict data which is continious such as Price of house\n",
        "\n",
        "\n",
        "\n",
        "Linear regression and logistic regression are both types of regression models used in statistics and machine learning, but they serve different purposes and are suitable for different types of data.\n",
        "\n",
        "Linear Regression:\n",
        "\n",
        "Purpose: Linear regression is used for predicting a continuous dependent variable based on one or more independent variables. It establishes a linear relationship between the predictor variables and the target variable.\n",
        "Output: The output of linear regression is a continuous value, making it suitable for tasks like predicting house prices, stock prices, or any other quantity that can vary over a range.\n",
        "Example: Predicting the price of a house based on its size, number of bedrooms, and other relevant features.\n",
        "\n",
        "Logistic Regression:\n",
        "\n",
        "Purpose: Logistic regression is used for predicting the probability of an event happening or not happening. It is commonly used for binary classification problems where the outcome is either 0 or 1 (or True/False, Yes/No, etc.).\n",
        "Output: The output of logistic regression is a probability score that is transformed using the logistic function, also known as the sigmoid function, which squashes the values between 0 and 1.\n",
        "Example: Predicting whether a student passes (1) or fails (0) an exam based on study hours, previous grades, and other relevant factors.\n",
        "\n",
        "Scenario where logistic regression is more appropriate:\n",
        "Consider a scenario where you want to predict whether an email is spam or not spam (ham). This is a binary classification problem because each email falls into one of two categories. Logistic regression is suitable for this task because it models the probability that an email is spam (versus not spam). The output of logistic regression can be interpreted as the likelihood of belonging to a particular class, and the decision threshold can be set to classify emails accordingly (e.g., if the probability is greater than 0.5, classify as spam)."
      ],
      "metadata": {
        "id": "RPBZkj5SOWoi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qtw1QjdVPZn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the cost function used in logistic regression, and how is it optimized?"
      ],
      "metadata": {
        "id": "lwh6XMlWPqyn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hAlpqUVVPrjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "In logistic regression, the cost function, also known as the logistic loss or cross-entropy loss, is used to measure the difference between the predicted probabilities and the actual class labels. The goal during training is to minimize this cost function.\n",
        "\n",
        "The logistic regression cost function for a single training example is given by:\n",
        "\n",
        "\n",
        "J(θ)=−[y⋅log(h\n",
        "θ\n",
        "​\n",
        " (x))+(1−y)⋅log(1−h\n",
        "θ\n",
        "​\n",
        " (x))]\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "J(θ) is the cost function.\n",
        "\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x) is the logistic (sigmoid) function applied to the linear combination of input features\n",
        "\n",
        "x with weights\n",
        "\n",
        "θ.\n",
        "\n",
        "y is the actual class label (0 or 1).\n",
        "The cost function penalizes the model more when the predicted probability is far from the true class label.\n",
        "\n",
        "The overall cost function for the entire dataset is the average of the individual cost functions over all training examples:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kmFqW7vBQBif"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xw8Pl1s8QJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
      ],
      "metadata": {
        "id": "-3IyzJJQRsna"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ug_g1kdPRtax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:\n",
        "\n",
        "\n",
        "Regularization is a method of preventing overfitting, which is a common problem in machine learning. Overfitting means that your model learns too much from the specific data you have, and fails to generalize well to new or unseen data. This can lead to poor predictions and low performance. Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "GRoEdXPvSAbS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBanNmIySSgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
      ],
      "metadata": {
        "id": "WtxDxSwnSVAR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUbeCauvSWp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classification model, such as logistic regression, at various threshold settings. It plots the True Positive Rate (Sensitivity or Recall) against the False Positive Rate at different classification thresholds. The area under the ROC curve (AUC-ROC) is commonly used as a summary metric to evaluate the overall performance of the model.\n",
        "\n",
        "Here's a breakdown of the key terms and concepts related to ROC curves:\n",
        "\n",
        "True Positive Rate (Sensitivity or Recall):\n",
        "\n",
        "This is the proportion of actual positive instances correctly classified by the model. It is calculated as:\n",
        "\n",
        "True Positive Rate\n",
        "=\n",
        "True Positives/\n",
        "(True Positives\n",
        "+\n",
        "False Negatives)\n",
        "\n",
        "\n",
        "\n",
        "False Positive Rate:\n",
        "\n",
        "This is the proportion of actual negative instances incorrectly classified as positive by the model. It is calculated as:\n",
        "False Positive Rate\n",
        "=\n",
        "False Positives/\n",
        "(False Positives\n",
        "+\n",
        "True Negatives)\n",
        "\n",
        "\n",
        "ROC Curve:\n",
        "\n",
        "The ROC curve is a graphical representation of the trade-off between sensitivity and specificity (or equivalently, true positive rate and false positive rate) at different classification thresholds. It is created by plotting the True Positive Rate against the False Positive Rate for various threshold values.\n",
        "AUC-ROC (Area Under the ROC Curve):\n",
        "\n",
        "AUC-ROC is a scalar value representing the area under the ROC curve. It provides a single metric to summarize the model's overall ability to discriminate between positive and negative instances. A model with higher AUC-ROC is considered to have better discriminatory power."
      ],
      "metadata": {
        "id": "bu7vBdAaSjPb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvqc3QghSlYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
      ],
      "metadata": {
        "id": "9W5tkkLlTxXG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWKB8egwTzQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "Feature selection is a crucial step in building a logistic regression model. It involves selecting a subset of relevant features from the original set of features to improve the model's performance, reduce overfitting, and enhance interpretability. Here are some common techniques for feature selection in  logistic regression:\n",
        "\n",
        "# Univariate Feature Selection:\n",
        "\n",
        "This method involves evaluating each feature independently to identify the ones that have the strongest relationship with the target variable. Common statistical tests, such as chi-square tests for categorical variables or t-tests for continuous variables, can be used to assess the significance of individual features.\n",
        "# Recursive Feature Elimination (RFE):\n",
        "\n",
        "RFE is an iterative method that starts with all features and gradually eliminates the least important ones. The logistic regression model is trained multiple times, and at each iteration, the least important features are removed based on their weights or importance scores.\n",
        "# L1 Regularization (Lasso):\n",
        "\n",
        "L1 regularization introduces a penalty term based on the absolute values of the regression coefficients. This penalty encourages sparsity in the model, leading some coefficients to become exactly zero. Features with zero coefficients are effectively ignored by the model.\n",
        "# L2 Regularization (Ridge):\n",
        "\n",
        "L2 regularization adds a penalty term based on the squared values of the regression coefficients. While it does not lead to exact sparsity like L1 regularization, it can still shrink the coefficients of less important features, reducing their impact on the model.\n",
        "# Feature Importance from Tree-Based Models:\n",
        "\n",
        "Ensemble methods like Random Forests or Gradient Boosting can provide feature importance scores. Features with higher importance scores are considered more influential in making predictions. These scores can guide feature selection in logistic regression.\n",
        "# Correlation-Based Feature Selection:\n",
        "\n",
        "Identify and remove features that are highly correlated with each other. High correlation between features might introduce redundancy, and removing one of them can improve model performance and interpretability.\n",
        "# Information Gain or Mutual Information:\n",
        "\n",
        "These metrics measure the dependence between a feature and the target variable. Features with high information gain or mutual information are considered more informative and can be selected for the model.\n",
        "# Benefits of Feature Selection:\n",
        "\n",
        "Improved Model Performance: By focusing on the most relevant features, the model can be more robust, generalizable, and less prone to overfitting.\n",
        "\n",
        "Reduced Overfitting: Feature selection helps prevent the model from learning noise or irrelevant patterns in the data, reducing the risk of overfitting.\n",
        "\n",
        "Computational Efficiency: A model with fewer features is often computationally less expensive to train and deploy.\n",
        "\n",
        "Interpretability: Models with fewer features are generally easier to interpret and explain to stakeholders.\n",
        "\n",
        "Avoidance of Multicollinearity Issues: Removing highly correlated features can enhance the stability of the model and improve its reliability.\n",
        "\n",
        "It's important to note that the choice of feature selection technique depends on the nature of the data and the specific goals of the modeling task. It's often a good practice to experiment with different methods and evaluate their impact on model performance using appropriate validation techniques"
      ],
      "metadata": {
        "id": "trnn-7YcUW8i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqrOarblUp-T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}