{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXxnDmR27Ymo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
      ],
      "metadata": {
        "id": "jbiSzTOC7fNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3GODefly7gPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-Ridge Regression is a linear regression technique that introduces regularization by adding a penalty term to the traditional least squares objective function. The regularization term is proportional to the square of the magnitude of the coefficients. This additional term helps prevent overfitting and can be particularly useful when dealing with multicollinearity, where predictor variables are highly correlated.Ridge Regression aims to minimize the sum of squared differences between the observed and predicted values while adding a penalty term that is proportional to the square of the coefficients."
      ],
      "metadata": {
        "id": "LsV0CMZA8N7v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVBREJmb8hDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What are the assumptions of Ridge Regression?"
      ],
      "metadata": {
        "id": "Pjdcou6S8j0i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GN5rCF128k8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "Ridge Regression shares many assumptions with ordinary least squares (OLS) regression, as they are both linear regression techniques. The basic assumptions include:\n",
        "\n",
        "Linearity: Ridge Regression assumes that the relationship between the dependent variable and the predictor variables is linear.\n",
        "\n",
        "Independence of Errors: The errors (residuals), which are the differences between the observed and predicted values, should be independent of each other. This assumption ensures that one observation's error does not provide information about the errors of other observations."
      ],
      "metadata": {
        "id": "dH8rX__i8xeC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pN8bEEsw8zqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
      ],
      "metadata": {
        "id": "ZHpg_MQJ80lO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k546I0i_85MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:- In Ridge Regression, the tuning parameter (位), also known as the regularization parameter, controls the strength of the regularization applied to the model. The selection of the optimal 位 is crucial for obtaining the best trade-off between fitting the data well and preventing overfitting.\n",
        "\n",
        "Cross-Validation:\n",
        "\n",
        "Use cross-validation techniques, such as k-fold cross-validation, to evaluate the model's performance for different values of 位.\n",
        "Select the 位 that provides the best trade-off between bias and variance, typically by minimizing the mean squared error (MSE) or another relevant metric on the validation set."
      ],
      "metadata": {
        "id": "g2g-aGap9JCz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMyWEgoX9O8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Can Ridge Regression be used for feature selection? If yes, how?"
      ],
      "metadata": {
        "id": "Bfl82_8R9c8r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HF6fZ8Ep9dzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-Yes, Ridge Regression can be used for feature selection to some extent, although it does not perform as aggressive feature selection as some other techniques like Lasso Regression. The regularization term in Ridge Regression, which is proportional to the square of the coefficients, tends to shrink the coefficients towards zero but rarely makes them exactly zero. However, the shrinkage effect can make some coefficients very small, effectively reducing the impact of certain features in the model."
      ],
      "metadata": {
        "id": "S-tqo3Jf9mB2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2hFD4dM9wVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
      ],
      "metadata": {
        "id": "RLdobMDe9-0B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwHk_xmd9_p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AANS:-Ridge Regression is particularly useful in the presence of multicollinearity, a situation where predictor variables in a regression model are highly correlated. Multicollinearity can lead to unstable and inflated coefficient estimates in ordinary least squares (OLS) regression. Ridge Regression helps address this issue through the introduction of a regularization term."
      ],
      "metadata": {
        "id": "_JWQVaKo-Hj1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMgCHPTo-J3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
      ],
      "metadata": {
        "id": "AAQ84uiY-PQh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Swz9hsX-Qca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}