{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3i-0rVBQmC0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
      ],
      "metadata": {
        "id": "VnrF-kyJQwbV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X7L92DvZQ0ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS :- The Lasso regression which   is also known as L1 regularization where we add penality term to the cost function so that we can handle the case of overfitting . in this we have Lambda*coffecients as the penality term . the less is the coffecient , the less that important feature is for that given independent variable . the more is the coffecient , the more important is the feature.it differs from ridge reression because there we have square of labda*coffecients as penality term"
      ],
      "metadata": {
        "id": "VYZosxQIo9gL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTSboqb9tSn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n"
      ],
      "metadata": {
        "id": "41u3MJwMtW7G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n_nEbfrtYXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "#Lasso Regression, or L1 regularization, is a linear regression technique that adds a penalty term based on the absolute values of the coefficients. The main advantage of using Lasso Regression in feature selection is its ability to perform automatic feature selection by driving some of the coefficients to exactly zero.\n",
        "\n",
        "#The L1 penalty term encourages sparsity in the model, meaning that it tends to set the coefficients of less important features to zero, effectively eliminating them from the model. This helps in feature selection by keeping only the most relevant features in the final model, making the model simpler and potentially improving its interpretability."
      ],
      "metadata": {
        "id": "4aSVPWottslF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ciXvBXmtxby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How do you interpret the coefficients of a Lasso Regression model?"
      ],
      "metadata": {
        "id": "H-wYjEWot1LH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86oVmfqvt2LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        " # If the coefficient of a feature is non-zero, it suggests that the corresponding feature is considered important by the model in predicting the target variable. The larger the coefficient, the greater the impact of that feature on the predicted outcome.\n",
        "\n",
        "\n",
        " # ___**__\n",
        "\n",
        "\n",
        "#Zero Coefficients: If the coefficient of a feature is exactly zero, it implies that the Lasso Regression has effectively excluded that feature from the model. This feature is deemed less important or irrelevant in predicting the target variable.\n",
        "\n",
        "#__***__\n",
        "\n",
        "\n",
        "\n",
        "#Magnitude of Coefficients: The magnitude of the non-zero coefficients provides a sense of the strength of the relationship between the features and the target variable. Larger absolute values indicate a stronger impact, while smaller values suggest a more moderate influence.\n",
        "\n",
        "\n",
        "#__**__\n",
        "\n",
        "#Regularization Strength: The regularization strength (controlled by the regularization parameter, often denoted as alpha) in Lasso Regression influences the degree of sparsity in the model. Higher values of alpha lead to more coefficients being driven to zero, resulting in a more sparse model with fewer features."
      ],
      "metadata": {
        "id": "5nQbqvCIuUMF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yk_AwrFnusZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
      ],
      "metadata": {
        "id": "RIvnKBJ3u0Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCEPgZhou2g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "\n",
        "# In Lasso Regression, the primary tuning parameter is the regularization parameter, often denoted as \"alpha\" (α). This parameter controls the strength of the penalty applied to the absolute values of the coefficients. The higher the alpha, the stronger the penalty, leading to more coefficients being driven towards zero. The tuning parameters and their effects on the model's performance are as follows:\n",
        "\n",
        "# __****___\n",
        "\n",
        "# _*_  Alpha (α):\n",
        "\n",
        "# Effect: It controls the strength of regularization. Higher alpha values result in more coefficients being exactly zero, leading to a sparser model.\n",
        "\n",
        "# __***__\n",
        "\n",
        "\n",
        "# Tuning: Cross-validation techniques, such as k-fold cross-validation, can be used to find the optimal alpha value that balances model complexity and performance.\n",
        "\n",
        "# __**__\n",
        "# __*__ Max Iterations:\n",
        "\n",
        "\n",
        "# Effect: Lasso Regression is typically solved using iterative optimization algorithms. The max iterations parameter specifies the maximum number of iterations allowed for the algorithm to converge.\n",
        "\n",
        "# __**__\n",
        "\n",
        "\n",
        "# Tuning: If the algorithm does not converge within the specified number of iterations, you may need to increase the max iterations.\n",
        "\n",
        "# __**__\n",
        "\n",
        "# __*__ Normalization:\n",
        "\n",
        "# Effect: Lasso Regression often includes normalization, where the features are scaled before applying the regularization. This ensures that all features contribute equally to the regularization penalty.\n",
        "\n",
        "# __**__\n",
        "\n",
        "# Tuning: It's usually advisable to keep normalization enabled to prevent features with large scales from dominating the regularization process."
      ],
      "metadata": {
        "id": "I-xhYDZHvHVp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXFlS3tzvuet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n"
      ],
      "metadata": {
        "id": "EeKJg27ev0px"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ka8Cfyerv3f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "#  Lasso Regression is inherently a linear regression technique and is not designed for handling non-linear relationships between features and the target variable. For non-linear regression problems, other methods such as polynomial regression, kernelized methods, or non-linear models like decision trees or neural networks are more appropriate"
      ],
      "metadata": {
        "id": "Vh5tuKpYwCS4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6u8u6GxwGOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. What is the difference between Ridge Regression and Lasso Regression?"
      ],
      "metadata": {
        "id": "H2nr_FttwKpu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmS_Y-9wwLhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ANS:-\n",
        "# while both Ridge and Lasso Regression introduce regularization to prevent overfitting, Lasso Regression stands out for its ability to perform feature selection by setting certain coefficients to zero, making it particularly useful when dealing with datasets with many features and a desire for a more interpretable and parsimonious model. Ridge Regression, on the other hand, tends to shrink coefficients toward each other without eliminating them entirely."
      ],
      "metadata": {
        "id": "czCtmPTuwgZh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZj1G614wjd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
        "\n",
        "\n",
        "# ANS:-\n",
        "# Yes, Lasso Regression has a built-in mechanism that allows it to handle multicollinearity in the input features to some extent. Multicollinearity refers to high correlations among predictor variables, which can cause issues in traditional linear regression models. Lasso Regression addresses multicollinearity in the following way:\n",
        "\n",
        "# Automatic Feature Selection:\n",
        "\n",
        "# Lasso Regression has a tendency to drive the coefficients of less important features exactly to zero.\n",
        "#When there is multicollinearity, where two or more features are highly correlated, Lasso Regression may choose one of them and drive the coefficients of the others to zero during the regularization process."
      ],
      "metadata": {
        "id": "uBq7ZEyqwxH1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTGfiOBdw9AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
      ],
      "metadata": {
        "id": "rajvBgnDxJQK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_vg5dpAxKWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:=The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen through cross-validation techniques, such as k-fold cross-validation. By evaluating the model's performance across different values of lambda, you can identify the one that balances model complexity and predictive accuracy, preventing overfitting or underfitting."
      ],
      "metadata": {
        "id": "InIOuhJvxLHd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGvoPYSPxe2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}