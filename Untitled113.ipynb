{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjielkMbvT_h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is boosting in machine learning?"
      ],
      "metadata": {
        "id": "C7Gr58vfwUWD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsQpwQSnwVF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "- Boosting is a machine learning ensemble technique that combines multiple weak learners (typically decision trees) to create a strong learner. The idea behind boosting is to sequentially train new models to correct errors made by previous models. Each new model focuses on instances that were previously misclassified, thereby improving the overall model's performance."
      ],
      "metadata": {
        "id": "hfPRtwwewhFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris"
      ],
      "metadata": {
        "id": "1mYqvplLwkUX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris=load_iris()"
      ],
      "metadata": {
        "id": "4jhCRmhYxCC7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=iris.data\n",
        "y=iris.target"
      ],
      "metadata": {
        "id": "jIXTRKGexGUH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "adaboost_clf=AdaBoostClassifier(n_estimators=50,random_state=42)"
      ],
      "metadata": {
        "id": "QR9lIY06xKc3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost_clf.fit(x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "lnMrKkBdxn7l",
        "outputId": "d5bcede7-8782-4843-be78-a9280f3b8522"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=adaboost_clf.predict(x_test)"
      ],
      "metadata": {
        "id": "U3pdMKkvxuXU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lla5P-cPxzEo",
        "outputId": "7ed1876d-2093-4812-9665-68d947a357fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "weFc_sCax2re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What are the advantages and limitations of using boosting techniques?"
      ],
      "metadata": {
        "id": "a9ip4bBjx8Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "atoP7-kNx9Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "# Boosting techniques, such as AdaBoost, Gradient Boosting Machines (GBM), and XGBoost, offer several advantages and have some limitations. Let's discuss them along with a code example using a real dataset.\n",
        "\n",
        "- Advantages of Boosting Techniques:\n",
        "- Improved Accuracy: Boosting algorithms typically achieve higher accuracy compared to individual weak learners.\n",
        "- Robustness to Overfitting: Boosting algorithms, especially when properly tuned, are less prone to overfitting due to their iterative nature.\n",
        "- Handles Imbalanced Data: Boosting methods can handle imbalanced datasets well by focusing on misclassified instances.\n",
        "Feature Importance: Boosting algorithms provide insights into feature importance, helping in feature selection and interpretation.\n",
        "- Versatility: Boosting algorithms can be used for both classification and regression tasks.\n",
        "- Limitations of Boosting Techniques:\n",
        "Sensitive to Noisy Data and Outliers: Boosting algorithms can be sensitive to noisy data and outliers, which may impact model performance.\n",
        "- Computationally Intensive: Training boosting models can be computationally expensive, especially with a large number of iterations and complex base learners.\n",
        "- Requires Tuning: Boosting algorithms require careful tuning of hyperparameters to achieve optimal performance.\n",
        "Not Interpretable: While boosting provides feature importance, the overall model may not be as interpretable as simpler models like decision trees.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cuJ2zLFyMFw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfyX_625yzNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Explain how boosting works."
      ],
      "metadata": {
        "id": "-UA5zsGhy8nP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5aurqmfy9rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "\n",
        "- Boosting is an ensemble learning technique that combines multiple weak learners to create a strong learner. Unlike bagging techniques (e.g., Random Forest), where models are trained independently in parallel, boosting trains models sequentially, with each new model focusing on instances that were previously misclassified by the ensemble. Here's how boosting works in detail:\n",
        "\n",
        "- Initialization:\n",
        "\n",
        "Boosting starts by initializing a base model, which is often a weak learner like a decision stump (a decision tree with only one split).\n",
        "- Sequential Training:\n",
        "\n",
        "The first base model is trained on the entire dataset.\n",
        "For subsequent models, the algorithm focuses on instances that were misclassified by the previous models. It assigns higher weights to misclassified instances and lower weights to correctly classified instances.\n",
        "- Weighted Training:\n",
        "\n",
        "In each iteration, the algorithm assigns weights to each training instance based on whether it was correctly or incorrectly classified by the ensemble of models so far.\n",
        "Misclassified instances get higher weights, making them more influential in the training of the next model.\n",
        "- Model Combination:\n",
        "\n",
        "After each model is trained, it is added to the ensemble, and the weights of all models are combined to make predictions.\n",
        "Typically, models with higher accuracy contribute more to the final prediction.\n",
        "- Iteration and Stopping Criteria:\n",
        "\n",
        "The process of training new models and updating weights iterates for a predefined number of iterations or until a stopping criteria is met (e.g., no further improvement in performance).\n",
        "- Final Prediction:\n",
        "\n",
        "- To make predictions, the boosting algorithm combines the predictions of all models in the ensemble, usually using a weighted sum or voting mechanism.\n"
      ],
      "metadata": {
        "id": "mmeH7yqyzOWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris_data = load_iris()\n",
        "X = iris_data.data\n",
        "y = iris_data.target\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an AdaBoost classifier with a decision tree base estimator\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IgFpEoHzdM-",
        "outputId": "504ca5b6-fbdc-4801-a916-79c5e3080a9e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtQD-ftLztIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What are the different types of boosting algorithms?"
      ],
      "metadata": {
        "id": "5Vc7ryS9zw1W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "buZqDsQHzx9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANS:-\n",
        "# There are several types of boosting algorithms, each with its own variations and characteristics. Some common types of boosting algorithms include:\n",
        "\n",
        "- AdaBoost (Adaptive Boosting):\n",
        "\n",
        "AdaBoost is one of the earliest and most popular boosting algorithms. It trains a series of weak learners (e.g., decision trees) sequentially, with each new model focusing on instances that were previously misclassified. AdaBoost adjusts the weights of misclassified instances to improve model performance.\n",
        "Example: Using AdaBoost with the Titanic dataset.\n",
        "- Gradient Boosting Machines (GBM):\n",
        "\n",
        "GBM is a more advanced boosting algorithm that builds trees in a sequential manner, where each new tree corrects errors made by the previous ones. GBM uses gradient descent optimization to minimize the loss function, making it suitable for regression and classification tasks.\n",
        "Example: Using Gradient Boosting Classifier with the Iris dataset.\n",
        "- XGBoost (Extreme Gradient Boosting):\n",
        "\n",
        "XGBoost is an optimized implementation of gradient boosting that introduces regularization techniques to prevent overfitting and improve performance. It supports parallel processing, tree pruning, and customizable hyperparameters.\n",
        "Example: Using XGBoost with the Boston Housing dataset.\n",
        "- LightGBM (Light Gradient Boosting Machine):\n",
        "\n",
        "LightGBM is another optimized implementation of gradient boosting that focuses on faster training speed and lower memory usage. It uses a histogram-based approach to split data and supports categorical features directly.\n",
        "Example: Using LightGBM with the Wine Quality dataset.\n",
        "- CatBoost:\n",
        "\n",
        "CatBoost is a boosting algorithm developed by Yandex that is designed to handle categorical features efficiently. It automatically handles categorical data encoding and incorporates techniques like ordered boosting and oblivious trees for improved performance."
      ],
      "metadata": {
        "id": "nPVbVQ_Iz9Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSa2K2O31hTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}